{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classification algorithm. Here, it will be used as a binary classifier, but it can also be expanded to multi-class classification.\n",
    "Most of the time the classes are labeled as zero and one. For example, in the example below malignant breast cancer cells are class one and benign breast cancer cells \n",
    "correspond to class zero. A linear combination of the features is still used like in linear regression. However, linear regression cannot be used as its hypothesis \n",
    "has a codomain of $\\cal{R}$, but $y_i$ can only have two values (zero or one) in logistic regression. This means that Logistic regression needs a function whose \n",
    "codomain is $(0, 1)$. A function that satisfies this constraint is the sigmoid function\n",
    "\n",
    "$$ h_\\theta (x_i) = \\frac{1}{1+e^{\\theta x_i}}$$\n",
    "\n",
    "In the exponential, the hypothesis of linear regression is found. The objective function of logistic regression is the log-likelihood:\n",
    "\n",
    "$$ - \\frac{1}{N} \\sum_i y_i \\ln\\left( h_\\theta(x_i) \\right) + \\left( 1 - y_i \\right) \\ln\\left(1-h_\\theta(x_i) \\right)$$\n",
    "\n",
    "Gradient descent is again used to optimize the objective function, using the same formula as linear regression.\n",
    "\n",
    "$$\n",
    "\\theta^{(j)} \\leftarrow \\theta^{(j)} - \\alpha \\frac{1}{N} \\sum_{i=0}^N \\left( h_{\\pmb{\\theta}} \\left( \\pmb{x}_i \\right) - y_i \\right ) x_i^{(j)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, theta):\n",
    "    return 1/(1+np.exp(-X@theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(theta, X, y):\n",
    "    return -np.sum(y*np.log(sigmoid(X, theta)) + (1-y)*np.log(1-sigmoid(X, theta)))/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/BreastCancer.csv\")\n",
    "df.drop(labels=[\"id\", \"Unnamed: 32\"], axis=1, inplace=True)\n",
    "\n",
    "# Change the labels M and B to 1 and 0 respectively\n",
    "df.replace(\"M\", 1, inplace=True)\n",
    "df.replace(\"B\", 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vector X and class vector y\n",
    "X = df.drop(labels=['diagnosis'], axis=1).to_numpy()\n",
    "y = df['diagnosis'].to_numpy()\n",
    "\n",
    "# Split the data into a 80% training and 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate, num_iterations, threshold=1e-3, blog=False):\n",
    "    \"\"\"\n",
    "    Use gradient descent to optimize regression parameters theta in order to find the best straight\n",
    "    line for the given points\n",
    "\n",
    "    :param points: points to fit the line\n",
    "    :param learning_rate: learning rate used in the algorithm\n",
    "    :param num_iterations: maximum number of iterations\n",
    "    :param threshold: minimum difference between two sequential mean squared\n",
    "        error values (default is 1e-3)\n",
    "    :param blog: indicates to print the cost at every step (default is False)\n",
    "    \"\"\"\n",
    "\n",
    "    # Init values\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    m = X_train.shape[0]\n",
    "    iteration = 0\n",
    "\n",
    "    J = loglikelihood(theta, X_train, y_train)\n",
    "    prev_J = np.inf\n",
    "\n",
    "    # Loop until convergence or maximum number of iterations is reached\n",
    "    while iteration < num_iterations and np.all(np.abs(J - prev_J) > threshold):\n",
    "\n",
    "        new_thetas = np.zeros(len(theta))\n",
    "        prev_J = J\n",
    "\n",
    "        # Compute new theta's using the gradient\n",
    "        for j, theta_j in enumerate(theta):\n",
    "            new_thetas[j] = theta_j - learning_rate / m * np.sum(\n",
    "                (sigmoid(X_train, theta) - y_train) * X_train[:, j]\n",
    "            )\n",
    "\n",
    "        theta = new_thetas\n",
    "        # Compute new MSE\n",
    "        J = loglikelihood(theta, X_train, y_train)\n",
    "\n",
    "        if blog:\n",
    "            print(f\"{iteration}: {J}\")\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return theta, J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.1765562847799052\n",
      "1: 0.14168152486288937\n",
      "2: 0.12597680293130076\n",
      "3: 0.11865746865556694\n",
      "4: 0.11392830594814492\n",
      "5: 0.11022378970399437\n",
      "6: 0.10712659074630146\n",
      "7: 0.10446078750151873\n",
      "8: 0.10212653791773602\n",
      "9: 0.10005751186805145\n",
      "10: 0.0982058485829798\n",
      "11: 0.09653536682323952\n",
      "12: 0.09501790144512029\n",
      "13: 0.09363107256453192\n",
      "14: 0.09235681553594174\n",
      "15: 0.09118036073870096\n",
      "16: 0.09008949998152911\n",
      "17: 0.08907404528814196\n",
      "18: 0.08812542167092427\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "\n",
    "theta_opt, J = gradient_descent(X_train_scaled, y_train, 1, 1e3, blog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "y_pred = [0 if h < 0.5 else 1 for h in sigmoid(X_test_scaled, theta_opt)]\n",
    "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a0ab666db3a5de9beeb0227a3e4f8e08a5e41653a682b5b11b0b57ebeb912a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
